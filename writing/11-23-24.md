## This is my first day doing deep learning

11/23/24

Today was mostly about getting the following script to run:

```python
import tensorflow as tf
import time

# Check if GPU is being used
print("Available GPUs:", tf.config.list_physical_devices('GPU'))

# Define a simple computation
matrix_size = 1000
a = tf.random.normal([matrix_size, matrix_size])
b = tf.random.normal([matrix_size, matrix_size])

# Perform the computation on GPU and measure time
start = time.time()
result = tf.matmul(a, b)
end = time.time()

print(f"Matrix multiplication on GPU took {end - start:.4f} seconds")

# Ensure the computation was performed on the GPU
if result.device.endswith('GPU:0'):
    print("Computation was performed on the GPU")
else:
    print("Computation was not performed on the GPU")
```

Where I got this output

```
Matrix multiplication on GPU took 0.0108 seconds
Computation was performed on the GPU
```

---

## Challenges

This is a day of many firsts for me, which made it difficult.

- First day with linux
- First day with Cuda
- First day with GPUs and nvidia

The following video ended up getting me unstuck

**TensorFlow GPU on Ubuntu 24.04** - https://www.youtube.com/watch?v=1Tr1ifuSh6o

I still don't understand most of the directions that the video spelled out for me. I wish there was a **linux for deep learning** type book I could read.

## Next Steps

Now that I have tensorflow working, it's time to move onto some real world examples.
